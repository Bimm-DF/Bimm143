---
title: "Class8"
author: "Diana Furlan"
format: pdf
---


Scale data before analysis, PCA:

#shows the first 6 rows, 'view shows full data of df-mtcars
```{r}
head(mtcars)
```
#gets the mean of every col of df mtcars
```{r}
colMeans(mtcars)
```
# looks for sd across (1 is use for rows and 2 for cols) of the matrix mtcars
```{r}
apply(mtcars, 2, sd)
```
#assigns x to scaled df mtcars #scale will calculate the vectors mean and sd
#head shows data like a "print"

```{r}
x<-scale(mtcars)
head(x)
```
```{r}
round(colMeans(x),2)
```

#Prep Data


```{r}
# code to input the data and store as wisc.df, data stored where the project was stored.
wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)

#display data  saved in wisc.df
head(wisc.df) 

```


#diagnosis  vectore created
```{r Q2}
diagnosis <- wisc.df[,1]
#to cound how many of a repeated data inpuded in df we use 'table'
table(diagnosis)
```
Remove this first 'diagnosis' column from ds, so it wont display to PCA. It is the expert ans to compare analysis.
```{r}
# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]
head(wisc.df)
```

```{r q3}

( grep("_mean", colnames(wisc.data)))
```

##Exploratory data analysis

>Q1. How many observations are in this dataset?569
>Q2. How many of the observations have a malignant diagnosis? 212
>Q3. How many variables/features in the data are suffixed with _mean? 10


##Principal Component Analysis

# Perform PCA on wisc.data(df), to retain most important information 

```{r}
wisc.pr <- prcomp( wisc.data, scale = T )
summary(wisc.pr)

```

>Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?0.447
Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?3
Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?7


```{r}
attributes(wisc.pr)

```
```{r}
biplot(wisc.pr)
```

>Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why? is unreadable, difficult to understand everything is on top of each other


```{r}
head(wisc.pr$x)
```
# Check column means and standard deviations
```{r}
colMeans(wisc.data)

apply(wisc.data,2,sd)

```



PC1vsPC20

```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,2],col=as.factor(diagnosis), xlab = "PC1", ylab = "PC2")
```
>Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?more organize you can see which ones are malignant and which ones are benign. readable.

PC1 vs Pc3
```{r}
plot(wisc.pr$x[,1], wisc.pr$x[,3],col=as.factor(diagnosis), xlab = "PC1", ylab = "PC3")

```

ggplot 

```{r}
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis
```

```{r}
library(ggplot2)
```
```{r}
ggplot(df) +
  aes(PC1, PC2, col= diagnosis) +
  geom_point()
```


##Variance Explained



```{r}
#standard dev sq
pr.var <- wisc.pr$sdev^2 
head(pr.var)
```


```{r}
#variance per component 'pve'
pve <- pr.var/sum(pr.var) 
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,1), typ="o")
```
```{r}
barplot(pve, ylab = "Percent of Variance Explained", 
        names.arg=paste0("PC", 1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

##Communicating PCA results

>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? it shows the relation between principal component in original data and concave.point_mean, -0.26085376 is the strenght of the contribution higher values would have a bigger impact on PC1.

```{r}
wisc.pr$rotation[, 1]
```


>Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?5

```{r}
min <-(cumsum(pve))
min

```

```{r}
mincomp <- which(min >= 0.80)[1]
mincomp
```

##Hierarchical clustering


```{r}
data.scaled <- scale(wisc.data)

```

```{r}
data.dist <- dist(data.scaled)
```

```{r}
wisc.hclust <- hclust(data.dist, method= "complete")
```

##Results of hierarchical clustering

>Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r  q11}
plot(wisc.hclust)
abline(h=19, col="red")
```

##Selecting number of clusters

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k=4)
table(wisc.hclust.clusters, diagnosis)

```
>Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10? 

```{r}
rage <- 2:10
dif <- lapply(rage, function(k) {
  wisc.hclust.clusters <- cutree(wisc.hclust, k = k)
  table(wisc.hclust.clusters, diagnosis)
})

dif
```
>Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.Maybe ward.d2 because it compacts clusters, it looks clean easy to read.


## Combining methods

```{r}
d<- dist( wisc.pr$x[,1:3])
hc<- hclust(d, method="ward.D2")
plot(hc)
```

```{r}
grps<-cutree(hc, k=2) #the number of a patient is in certain cluster (1 or 2 in this case)
table(grps)
```


```{r}
plot(wisc.pr$x, col=grps)
```

compare clusting results (in grps) to the expert diagnosis

```{r}

table(diagnosis)
```
```{r}

table(grps)
```

```{r}
table(diagnosis, grps)#combines both tables  starting with the clusters in diagnosis 1, 2
```

>Q15. How well does the newly created model with four clusters separate out the two diagnoses? Is more compact and clear

##Sensitivity

>Q17. Which of your analysis procedures resulted in a clustering model with the best specificity? hclist How about sensitivity? kmeans

##Prediction


```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}
plot(wisc.pr$x[,1:2], col=grps)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```



>Q18. Which of these new patients should we prioritize for follow up based on your results? Patient 1 

